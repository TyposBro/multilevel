# {PATH_TO_PROJECT}/docker-compose.yaml

services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile.dev
    container_name: backend_api
    ports:
      - "3000:3000"
    env_file:
      - ./api/.env
    environment:
      NODE_ENV: development
      PORT: "3000"
      KOKORO_PREPROCESS_URL: "http://kokoro_preprocessor:8000/preprocess"

    # --- Live Reloading Setup ---
    command: npm run dev
    volumes:
      # Mount local code to /app (matching WORKDIR in Dockerfile.dev)
      - ./api:/app
      # Anonymous volume to keep container's node_modules separate
      - /app/node_modules

    depends_on:
      - kokoro_preprocessor
    networks:
      - app_network
  # tts_service: # This is the FastKoko TTS service
  # image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
  # deploy:
  #   resources:
  #     reservations:
  #       devices:
  #         - driver: nvidia
  #           count: all
  #           capabilities: [gpu]
  # image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
  # container_name: fastkoko_tts_service
  # ports:
  #   - "8880:8880" # Host:Container
  # networks:
  #   - app_network
  # restart: unless-stopped
  # volumes:
  #   - ./fastkoko_models:/app/api/src/models
  #   - ./fastkoko_voicepacks:/app/api/src/data/voice_exports

  kokoro_preprocessor: # Our new Python FastAPI service for preprocessing
    build:
      context: ./kokoro_preprocess_service # Path to our Python FastAPI app
      dockerfile: Dockerfile.dev
    container_name: kokoro_preprocess_api_service
    # Optional: Expose port to host for direct testing if needed
    # ports:
    #   - "8100:8000" # Example: Map container's 8000 to host's 8100
    networks:
      - app_network
    restart: unless-stopped
    # No HF_HOME volume needed as configs are bundled
    # No device specific environment variables needed as it's CPU only for preprocessing
    # If your kokoro.pipeline still has a fallback to hf_hub_download for unbundled configs,
    # you might want to add a volume for that cache if you anticipate using it.
    # volumes:
    #   - kokoro_preprocessor_hf_cache:/app/huggingface_cache # Only if KPipeline has hf_hub_download fallback AND you want to cache those
    # environment: # Not strictly needed if KPipeline loads bundled configs and has no other env dependencies
    # - PYTORCH_ENABLE_MPS_FALLBACK=1 # If you were to run torch operations and wanted MPS locally

networks:
  app_network:
    driver: bridge
# Optional volume definition if you re-enable HF cache fallback in kokoro_preprocessor
# volumes:
#   kokoro_preprocessor_hf_cache:
